from __future__ import annotations

import math
import uuid
import wave
from pathlib import Path
from collections import deque
from threading import Lock
from typing import Deque, Dict, Any, List, Optional, Tuple

import numpy as np
import streamlit as st
import sys
from streamlit_webrtc import (
    WebRtcMode,
    RTCConfiguration,
    webrtc_streamer,
    AudioProcessorBase,
)

PROJECT_ROOT = Path(__file__).resolve().parents[2]
SERVER_ROOT = PROJECT_ROOT / "server"
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
if str(SERVER_ROOT) not in sys.path:
    sys.path.insert(0, str(SERVER_ROOT))

from server.utils.openai_audio import synthesize_speech, transcribe_audio

RECORDINGS_DIR = SERVER_ROOT / "data" / "interview_recordings"
RECORDINGS_DIR.mkdir(parents=True, exist_ok=True)

RTC_CONFIG = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)


def _init_context() -> Dict[str, Any]:
    """Interview Live í™”ë©´ì—ì„œ ì‚¬ìš©í•  ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜."""
    default_context = {
        "interview_id": None,
        "application_id": None,
        "candidate_name": "ì§€ì›ì",
        "role": "candidate",
        "origin_nav": "status",
        "current_question": 1,
        "total_questions": 5,
        "question_text": "AI ë©´ì ‘ê´€ì´ ì²« ì§ˆë¬¸ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "time_limit": 90,
        "transcript": [],
        "last_recording_path": None,
        "last_transcript": "",
    }
    ctx = st.session_state.setdefault("interview_live_context", default_context)

    # default keys ë³´ì¥
    for key, value in default_context.items():
        ctx.setdefault(key, value)
    return ctx


def _render_preflight_steps(ctx: Dict[str, Any]) -> None:
    st.title("AI ë©´ì ‘ì„ ì‹œì‘í•˜ê¸° ì „ì—â€¦")
    st.caption("ì›í™œí•œ ë©´ì ‘ ì§„í–‰ì„ ìœ„í•´ ì•„ë˜ ë‹¨ê³„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    steps = [
        ("Step 1", "ì¹´ë©”ë¼ / ë§ˆì´í¬ ì—°ê²° í…ŒìŠ¤íŠ¸"),
        ("Step 2", "ë©´ì ‘ ê·œì¹™ í™•ì¸ (ì œí•œ ì‹œê°„, ì¬ì‹œë„ ë¶ˆê°€ ë“±)"),
        ("Step 3", "ì¡°ìš©í•œ í™˜ê²½ì—ì„œ ì§„í–‰ ì¤€ë¹„"),
    ]

    cols = st.columns(len(steps))
    for col, (title, desc) in zip(cols, steps):
        with col:
            st.markdown(
                f"""
                <div style="
                    border:1px solid rgba(148,163,184,0.4);
                    border-radius:12px;
                    padding:16px;
                    background:rgba(15,23,42,0.75);
                    min-height:150px;
                ">
                    <p style="font-weight:700;color:#f9fafb;font-size:1.05rem;margin-bottom:6px;">{title}</p>
                    <p style="color:#cbd5f5;font-size:0.9rem;line-height:1.4;">{desc}</p>
                </div>
                """,
                unsafe_allow_html=True,
            )

    st.markdown("---")
    button_cols = st.columns([1, 1])
    with button_cols[0]:
        if st.button("â†©ï¸ ì´ì „ í™”ë©´", type="secondary", use_container_width=True):
            origin = ctx.get("origin_nav", "status")
            st.session_state["interview_live_started"] = False
            st.session_state["nav_selected_code"] = origin
            st.rerun()
    with button_cols[1]:
        if st.button("âœ… ë©´ì ‘ ì‹œì‘í•˜ê¸°", type="primary", use_container_width=True):
            st.session_state["interview_live_started"] = True
            st.rerun()


def _render_timer_html(seconds: int) -> str:
    minutes = math.floor(seconds / 60)
    remain = seconds % 60
    return f"<h3 style='text-align: right; color: #38bdf8; margin:0;'>â± {minutes:02d}:{remain:02d}</h3>"


class InterviewAudioProcessor(AudioProcessorBase):
    """ì›¹ìº ì—ì„œ ìˆ˜ì‹ í•œ ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ ë²„í¼ì— ì ì¬í•˜ê³  í•„ìš” ì‹œ ì¶”ì¶œ."""

    def __init__(self) -> None:
        self._buffer: Deque[np.ndarray] = deque(maxlen=1600)
        self._sample_rate: Optional[int] = None
        self._channels: Optional[int] = None
        self._lock = Lock()

    def recv(self, frame):
        self._sample_rate = frame.sample_rate
        self._channels = len(frame.layout.names)
        arr = frame.to_ndarray()
        with self._lock:
            self._buffer.append(arr.copy())
        return frame

    def dump_audio(self) -> Tuple[List[np.ndarray], int]:
        with self._lock:
            if not self._buffer:
                return [], 0
            arrays = list(self._buffer)
            self._buffer.clear()
        if not self._sample_rate:
            return [], 0
        return arrays, self._sample_rate


def render_interview_live_page() -> None:
    """AI ë©´ì ‘ ì‹¤ì‹œê°„ í™”ë©´ ë Œë”ë§."""
    ctx = _init_context()
    started = st.session_state.get("interview_live_started", False)

    st.markdown(
        """
        <style>
        .interview-frame {
            border:1px solid rgba(148,163,184,0.35);
            border-radius:16px;
            padding:16px;
            background:rgba(15,23,42,0.92);
            min-height:320px;
            display:flex;
            flex-direction:column;
            gap:12px;
            position:relative;
        }
        .interview-frame.ai::after {
            content:"AI ë©´ì ‘ê´€";
            position:absolute;
            top:12px;
            right:12px;
            font-size:0.8rem;
            color:#67e8f9;
            letter-spacing:0.05em;
        }
        .interview-frame.candidate::after {
            content:"ì§€ì›ì";
            position:absolute;
            top:12px;
            right:12px;
            font-size:0.8rem;
            color:#fbcfe8;
            letter-spacing:0.05em;
        }
        .webrtc-placeholder {
            flex:1;
            border-radius:12px;
            background:rgba(15,23,42,0.8);
            border:1px dashed rgba(148,163,184,0.5);
            display:flex;
            align-items:center;
            justify-content:center;
            color:#cbd5f5;
            font-size:0.95rem;
            text-align:center;
            padding:12px;
        }
        .ai-active {
            box-shadow:0 0 25px rgba(45,212,191,0.35);
            border-color:rgba(45,212,191,0.6) !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    if not started:
        _render_preflight_steps(ctx)
        return

    st.title("AI ëª¨ì˜ ë©´ì ‘ ëŒ€ì‹œë³´ë“œ")

    # ìƒë‹¨ ì œì–´ ë°”
    header_cols = st.columns([1.1, 2.0, 0.9])
    with header_cols[0]:
        origin = ctx.get("origin_nav", "status")
        if st.button("ğŸ”´ ë©´ì ‘ ì¢…ë£Œ", type="secondary", use_container_width=True):
            st.session_state["interview_live_started"] = False
            st.session_state["nav_selected_code"] = origin
            st.rerun()

    with header_cols[1]:
        progress_val = ctx["current_question"] / max(ctx["total_questions"], 1)
        st.progress(progress_val, text=f"ì§„í–‰ë¥ : {ctx['current_question']}/{ctx['total_questions']} ì§ˆë¬¸")
        st.metric("í˜„ì¬ ì§ˆë¬¸", f"Q{ctx['current_question']:02d}", ctx.get("question_text", ""))

    with header_cols[2]:
        st.markdown(_render_timer_html(ctx.get("time_limit", 90)), unsafe_allow_html=True)

    st.markdown("---")

    # ë“€ì–¼ ë·°
    video_cols = st.columns([1.2, 0.05, 1.2])
    with video_cols[0]:
        st.subheader("ğŸ¤– AI ë©´ì ‘ê´€")
        st.markdown(
            """
            <div class="interview-frame ai ai-active">
                <div class="webrtc-placeholder">
                    WebRTC ì»´í¬ë„ŒíŠ¸ ìë¦¬<br/>
                    (AI ì•„ë°”íƒ€ ìŠ¤íŠ¸ë¦¼ / TTS ì¶œë ¥)
                </div>
                <div style="margin-top:8px;">
                    <p style="margin:0;font-weight:600;color:#67e8f9;">í˜„ì¬ ì§ˆë¬¸</p>
                    <p style="margin:4px 0 0;color:#e0f2fe;font-size:1rem;">{question}</p>
                </div>
            </div>
            """.format(question=ctx.get("question_text", "")),
            unsafe_allow_html=True,
        )

    with video_cols[2]:
        st.subheader("ğŸ‘¤ ì§€ì›ì")
        st.markdown(
            """
            <div class="interview-frame candidate">
                <div class="webrtc-placeholder">
                    ì§€ì›ì ì¹´ë©”ë¼/ë§ˆì´í¬ ì—°ê²° ì¤‘...
                </div>
                <p style="text-align:center;color:#cbd5f5;margin:8px 0 0;font-size:0.9rem;">
                    * ë³¸ì¸ì˜ ìì„¸, í‘œì •, ì‹œì„ ì„ í™•ì¸í•˜ë©° ë‹µë³€ì„ ì§„í–‰í•´ì£¼ì„¸ìš”.
                </p>
            </div>
            """,
            unsafe_allow_html=True,
        )
        webrtc_ctx = webrtc_streamer(
            key="candidate-stream",
            mode=WebRtcMode.SENDRECV,
            rtc_configuration=RTC_CONFIG,
            media_stream_constraints={"video": True, "audio": True},
            audio_receiver_size=1024,
            async_processing=True,
            audio_processor_factory=InterviewAudioProcessor,
        )

        connection_ready = bool(webrtc_ctx and webrtc_ctx.state.playing)
        if connection_ready:
            st.success("ì¹´ë©”ë¼/ë§ˆì´í¬ ì—°ê²°ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.info("ì—°ê²° ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì ‘ê·¼ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.")

    st.markdown("---")

    # ì‹¤ì‹œê°„ STT
    st.subheader("ğŸ“ ì‹¤ì‹œê°„ ë‹µë³€ ê¸°ë¡ (STT)")
    st.caption("ğŸ‘‚ AIê°€ ì‚¬ìš©ìì˜ ë‹µë³€ì„ ì¸ì‹í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

    transcript_placeholder = st.empty()
    transcript_text = "\n".join(ctx.get("transcript") or ["(ì•„ì§ ìŒì„±ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.)"])
    transcript_placeholder.code(transcript_text, language="text")

    st.markdown(
        "<p style='color:#f87171;'>* ë‹µë³€ì´ ì™„ë£Œë˜ë©´ 'ë…¹ìŒ ì €ì¥ ë° STT' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</p>",
        unsafe_allow_html=True,
    )

    st.markdown("#### ğŸ™ï¸ ë‹µë³€ ë…¹ìŒ / STT")
    col_rec, col_tts = st.columns([1, 1])
    with col_rec:
        record_disabled = not connection_ready
        if st.button("ğŸ’¾ ë…¹ìŒ ì €ì¥ ë° STT", use_container_width=True, disabled=record_disabled):
            processor = getattr(webrtc_ctx, "audio_processor", None)
            if not isinstance(processor, InterviewAudioProcessor):
                st.error("ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œë¥¼ ì´ˆê¸°í™”í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            else:
                chunks, sample_rate = processor.dump_audio()
                if not chunks or not sample_rate:
                    st.warning("ìˆ˜ì‹ ëœ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ëª‡ ì´ˆê°„ ë§í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                else:
                    try:
                        file_path = _save_audio_chunks(chunks, sample_rate)
                    except ValueError as exc:
                        st.error(f"ë…¹ìŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {exc}")
                    else:
                        ctx["last_recording_path"] = str(file_path)
                        st.success(f"ë…¹ìŒ íŒŒì¼ ì €ì¥: {file_path.name}")
                        try:
                            text = transcribe_audio(file_path)
                            ctx["last_transcript"] = text
                            ctx.setdefault("transcript", []).append(text or "(ì¸ì‹ ì‹¤íŒ¨)")
                            transcript_placeholder.code("\n".join(ctx["transcript"]), language="text")
                            st.success("STT ê²°ê³¼ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        except Exception as exc:
                            st.error(f"STT ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {exc}")

    with col_tts:
        if st.button("ğŸ”Š AI ì§ˆë¬¸ ìŒì„± ì¬ìƒ", use_container_width=True):
            try:
                audio_bytes = synthesize_speech(ctx.get("question_text", ""))
                st.audio(audio_bytes, format="audio/wav")
            except Exception as exc:
                st.error(f"TTS ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {exc}")

    action_cols = st.columns([1, 1])
    with action_cols[0]:
        st.button("â¸ï¸ ì¼ì‹œ ì •ì§€", use_container_width=True, disabled=True)
    with action_cols[1]:
        st.button("ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™ >>", type="primary", use_container_width=True, disabled=True)


def _save_audio_chunks(chunks: List[np.ndarray], sample_rate: int) -> Path:
    """AudioProcessorì—ì„œ ì¶”ì¶œí•œ numpy ë°°ì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥."""
    if not chunks or not sample_rate:
        raise ValueError("ì˜¤ë””ì˜¤ ë²„í¼ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ìƒ˜í”Œë ˆì´íŠ¸ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    channels = chunks[0].shape[0]
    total_samples = sum(chunk.shape[1] for chunk in chunks)
    if total_samples < sample_rate * 0.5:
        raise ValueError("ë…¹ìŒ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 0.5ì´ˆ ì´ìƒ ë§í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    audio_ndarray = np.concatenate(chunks, axis=1)
    audio_ndarray = audio_ndarray.transpose()
    audio_ndarray = np.clip(audio_ndarray, -32768, 32767).astype(np.int16)

    file_path = RECORDINGS_DIR / f"{uuid.uuid4().hex}.wav"
    with wave.open(str(file_path), "wb") as wf:
        wf.setnchannels(channels)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_ndarray.tobytes())

    return file_path

