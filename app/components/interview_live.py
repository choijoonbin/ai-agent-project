from __future__ import annotations

import math
import time
import uuid
import wave
import requests
from pathlib import Path
from collections import deque
from threading import Lock
from typing import Deque, Dict, Any, List, Optional, Tuple

import numpy as np
import streamlit as st
import sys
from streamlit_webrtc import (
    WebRtcMode,
    RTCConfiguration,
    webrtc_streamer,
    AudioProcessorBase,
)

PROJECT_ROOT = Path(__file__).resolve().parents[2]
SERVER_ROOT = PROJECT_ROOT / "server"
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
if str(SERVER_ROOT) not in sys.path:
    sys.path.insert(0, str(SERVER_ROOT))

from server.utils.openai_audio import transcribe_audio

RECORDINGS_DIR = SERVER_ROOT / "data" / "interview_recordings"
RECORDINGS_DIR.mkdir(parents=True, exist_ok=True)

RTC_CONFIG = RTCConfiguration(
    {
        "iceServers": [
            {"urls": ["stun:stun.l.google.com:19302"]},
            {"urls": ["stun:stun1.l.google.com:19302"]},
            {"urls": ["stun:stun2.l.google.com:19302"]},
        ]
    }
)

# Backend API URL
BACKEND_URL = "http://localhost:9898"


# ========== API Helper Functions ========== #

def _start_interview_session(application_id: int, candidate_name: str, job_title: str, 
                             jd_text: str, resume_text: str, total_questions: int = 5) -> Dict[str, Any]:
    """ë©´ì ‘ ì„¸ì…˜ ì‹œì‘ API í˜¸ì¶œ"""
    try:
        response = requests.post(
            f"{BACKEND_URL}/api/v1/interview-live/start",
            json={
                "application_id": application_id,
                "candidate_name": candidate_name,
                "job_title": job_title,
                "jd_text": jd_text,
                "resume_text": resume_text,
                "total_questions": total_questions,
                "enable_rag": True,
            },
            timeout=180,  # 3ë¶„ìœ¼ë¡œ ì¦ê°€ (JD/Resume ë¶„ì„ + ì§ˆë¬¸ ìƒì„±)
        )
        response.raise_for_status()
        return response.json()
    except requests.exceptions.Timeout:
        st.error("ë©´ì ‘ ì¤€ë¹„ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return {}
    except requests.exceptions.RequestException as e:
        st.error(f"ë©´ì ‘ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}


def _submit_answer(session_id: str, answer: str, retry: int = 2) -> Dict[str, Any]:
    """ë‹µë³€ ì œì¶œ ë° ë‹¤ìŒ ì§ˆë¬¸ ë°›ê¸° API í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)"""
    last_error = None
    
    for attempt in range(retry + 1):
        try:
            response = requests.post(
                f"{BACKEND_URL}/api/v1/interview-live/submit-answer",
                json={
                    "session_id": session_id,
                    "answer": answer,
                },
                timeout=60,
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.ConnectionError as e:
            last_error = e
            if attempt < retry:
                st.warning(f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨. ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{retry})")
                time.sleep(1)
            continue
        except requests.exceptions.Timeout as e:
            last_error = e
            st.error("ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼. ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return {}
        except requests.exceptions.RequestException as e:
            last_error = e
            break
    
    st.error(f"ë‹µë³€ ì œì¶œ ì‹¤íŒ¨: {last_error}")
    st.caption("ğŸ’¡ ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš” (http://localhost:9898)")
    return {}


def _end_interview(session_id: str) -> Dict[str, Any]:
    """ë©´ì ‘ ì¢…ë£Œ API í˜¸ì¶œ"""
    try:
        response = requests.post(
            f"{BACKEND_URL}/api/v1/interview-live/end",
            json={"session_id": session_id},
            timeout=60,
        )
        response.raise_for_status()
        return response.json()
    except requests.exceptions.ConnectionError as e:
        st.error("ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return {}
    except requests.exceptions.Timeout:
        st.error("ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼. í‰ê°€ ìƒì„±ì— ì‹œê°„ì´ ê±¸ë¦¬ê³  ìˆìŠµë‹ˆë‹¤.")
        return {}
    except requests.exceptions.RequestException as e:
        st.error(f"ë©´ì ‘ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}


def _init_context() -> Dict[str, Any]:
    """Interview Live í™”ë©´ì—ì„œ ì‚¬ìš©í•  ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜."""
    default_context = {
        "session_id": None,  # ë°±ì—”ë“œ ë©´ì ‘ ì„¸ì…˜ ID
        "interview_id": None,
        "application_id": None,
        "candidate_name": "ì§€ì›ì",
        "job_title": "",
        "jd_text": "",
        "resume_text": "",
        "role": "candidate",
        "origin_nav": "status",
        "current_question": 0,
        "total_questions": 5,
        "question_text": "AI ë©´ì ‘ê´€ì´ ì²« ì§ˆë¬¸ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "question_category": "ì¼ë°˜",
        "time_limit": 90,
        "transcript": [],
        "last_recording_path": None,
        "last_transcript": "",
        "interview_started": False,  # ì‹¤ì œ ë©´ì ‘ ì‹œì‘ ì—¬ë¶€
    }
    ctx = st.session_state.setdefault("interview_live_context", default_context)

    # default keys ë³´ì¥
    for key, value in default_context.items():
        ctx.setdefault(key, value)
    return ctx


def _render_preflight_steps(ctx: Dict[str, Any]) -> None:
    st.title("AI ë©´ì ‘ì„ ì‹œì‘í•˜ê¸° ì „ì—â€¦")
    st.caption("ì›í™œí•œ ë©´ì ‘ ì§„í–‰ì„ ìœ„í•´ ì•„ë˜ ë‹¨ê³„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    steps = [
        ("Step 1", "ì¹´ë©”ë¼ / ë§ˆì´í¬ ì—°ê²° í…ŒìŠ¤íŠ¸"),
        ("Step 2", "ë©´ì ‘ ê·œì¹™ í™•ì¸ (ì œí•œ ì‹œê°„, ì¬ì‹œë„ ë¶ˆê°€ ë“±)"),
        ("Step 3", "ì¡°ìš©í•œ í™˜ê²½ì—ì„œ ì§„í–‰ ì¤€ë¹„"),
    ]

    cols = st.columns(len(steps))
    for col, (title, desc) in zip(cols, steps):
        with col:
            st.markdown(
                f"""
                <div style="
                    border:1px solid rgba(148,163,184,0.4);
                    border-radius:12px;
                    padding:16px;
                    background:rgba(15,23,42,0.75);
                    min-height:150px;
                ">
                    <p style="font-weight:700;color:#f9fafb;font-size:1.05rem;margin-bottom:6px;">{title}</p>
                    <p style="color:#cbd5f5;font-size:0.9rem;line-height:1.4;">{desc}</p>
                </div>
                """,
                unsafe_allow_html=True,
            )

    st.markdown("---")
    button_cols = st.columns([1, 1])
    with button_cols[0]:
        if st.button("â†©ï¸ ì´ì „ í™”ë©´", type="secondary", use_container_width=True):
            origin = ctx.get("origin_nav", "status")
            st.session_state["interview_live_started"] = False
            st.session_state["nav_selected_code"] = origin
            st.rerun()
    with button_cols[1]:
        if st.button("âœ… ë©´ì ‘ ì‹œì‘í•˜ê¸°", type="primary", use_container_width=True):
            # ë©´ì ‘ ì„¸ì…˜ ì‹œì‘ - ë°±ì—”ë“œ API í˜¸ì¶œ
            if not ctx.get("application_id"):
                st.error("ì§€ì›ì„œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ë©´ì ‘ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            with st.spinner("ë©´ì ‘ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤... (ì²« ì§ˆë¬¸ì„ ìƒì„± ì¤‘)"):
                result = _start_interview_session(
                    application_id=ctx["application_id"],
                    candidate_name=ctx["candidate_name"],
                    job_title=ctx.get("job_title", ""),
                    jd_text=ctx.get("jd_text", ""),
                    resume_text=ctx.get("resume_text", ""),
                    total_questions=ctx["total_questions"],
                )
            
            if result and "session_id" in result:
                # ì„¸ì…˜ ì •ë³´ ì €ì¥
                ctx["session_id"] = result["session_id"]
                ctx["question_text"] = result["first_question"]
                ctx["question_category"] = result.get("question_category", "ì¼ë°˜")
                ctx["current_question"] = result["current_question_num"]
                ctx["total_questions"] = result["total_questions"]
                ctx["interview_started"] = True
                
                st.session_state["interview_live_started"] = True
                st.success(f"âœ… ë©´ì ‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤! (ì„¸ì…˜ ID: {result['session_id'][:8]}...)")
                st.rerun()
            else:
                st.error("ë©´ì ‘ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°±ì—”ë“œ ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")


def _render_timer_html(seconds: int) -> str:
    """íƒ€ì´ë¨¸ HTML ë Œë”ë§ (ì •ì )"""
    minutes = math.floor(seconds / 60)
    remain = seconds % 60
    return f"<h3 style='text-align: right; color: #38bdf8; margin:0;'>â± {minutes:02d}:{remain:02d}</h3>"


def _render_countdown_timer(time_limit: int, key: str = "timer") -> None:
    """ì‹¤ì‹œê°„ ì¹´ìš´íŠ¸ë‹¤ìš´ íƒ€ì´ë¨¸ ë Œë”ë§"""
    # íƒ€ì´ë¨¸ ì‹œì‘ ì‹œê°„ ì´ˆê¸°í™”
    timer_key = f"timer_start_{key}"
    if timer_key not in st.session_state:
        st.session_state[timer_key] = time.time()
    
    # ê²½ê³¼ ì‹œê°„ ê³„ì‚°
    elapsed = int(time.time() - st.session_state[timer_key])
    remaining = max(time_limit - elapsed, 0)
    
    minutes = remaining // 60
    seconds = remaining % 60
    
    # ìƒ‰ìƒ: ì‹œê°„ì— ë”°ë¼ ë³€ê²½
    if remaining > 30:
        color = "#38bdf8"  # íŒŒë€ìƒ‰
    elif remaining > 10:
        color = "#fbbf24"  # ë…¸ë€ìƒ‰
    else:
        color = "#f87171"  # ë¹¨ê°„ìƒ‰
    
    st.markdown(
        f"<h3 style='text-align: right; color: {color}; margin:0;'>â± {minutes:02d}:{seconds:02d}</h3>",
        unsafe_allow_html=True
    )


class InterviewAudioProcessor(AudioProcessorBase):
    """ì›¹ìº ì—ì„œ ìˆ˜ì‹ í•œ ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ ë²„í¼ì— ì ì¬í•˜ê³  í•„ìš” ì‹œ ì¶”ì¶œ."""

    # í´ë˜ìŠ¤ ë ˆë²¨ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
    _instance: Optional["InterviewAudioProcessor"] = None

    def __init__(self) -> None:
        super().__init__()  # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” ëª…ì‹œ
        self._buffer: Deque[np.ndarray] = deque(maxlen=1600)
        self._sample_rate: Optional[int] = None
        self._channels: Optional[int] = None
        self._lock = Lock()
        self._frame_count = 0  # ë””ë²„ê¹…ìš© ì¹´ìš´í„°
        InterviewAudioProcessor._instance = self
        print(f"ğŸ”§ [DEBUG] InterviewAudioProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨: {id(self)}")

    def recv(self, frame):
        """
        ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹  ë° ë²„í¼ ì €ì¥
        - WebRTCê°€ ê¸°ë³¸ì ìœ¼ë¡œ í˜¸ì¶œí•˜ëŠ” ë©”ì„œë“œ
        """
        try:
            self._sample_rate = frame.sample_rate
            # PyAV AudioLayoutì—ì„œ ì±„ë„ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            if hasattr(frame.layout, 'channels'):
                self._channels = frame.layout.channels
            elif hasattr(frame.layout, 'nb_channels'):
                self._channels = frame.layout.nb_channels
            else:
                # ê¸°ë³¸ê°’: ìŠ¤í…Œë ˆì˜¤
                self._channels = 2
                
            arr = frame.to_ndarray()
            with self._lock:
                self._buffer.append(arr.copy())
                self._frame_count += 1
                # ì²˜ìŒ 10ê°œ í”„ë ˆì„ì€ ë¡œê·¸ ì¶œë ¥
                if self._frame_count <= 10:
                    print(f"ğŸ¤ [DEBUG] í”„ë ˆì„ ìˆ˜ì‹ ë¨ #{self._frame_count}: shape={arr.shape}, rate={self._sample_rate}, channels={self._channels}")
            
            return frame
        except Exception as e:
            print(f"âŒ [DEBUG] recv() ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return frame

    def dump_audio(self) -> Tuple[List[np.ndarray], int, int]:
        """ë²„í¼ì˜ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  í”„ë ˆì„ ì¹´ìš´íŠ¸ë„ ë°˜í™˜"""
        with self._lock:
            if not self._buffer:
                return [], 0, self._frame_count
            arrays = list(self._buffer)
            self._buffer.clear()
            frame_count = self._frame_count
            self._frame_count = 0  # ì¹´ìš´í„° ë¦¬ì…‹
        if not self._sample_rate:
            return [], 0, frame_count
        print(f"ğŸ“¤ [DEBUG] dump_audio() í˜¸ì¶œë¨: {len(arrays)}ê°œ ì²­í¬, {frame_count}ê°œ í”„ë ˆì„")
        return arrays, self._sample_rate, frame_count

    @classmethod
    def get_instance(cls) -> Optional["InterviewAudioProcessor"]:
        """í˜„ì¬ í™œì„±í™”ëœ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        return cls._instance


def create_audio_processor():
    """
    AudioProcessor Factory í•¨ìˆ˜
    - ì‹±ê¸€í†¤ íŒ¨í„´: ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
    """
    print(f"ğŸ­ [DEBUG] create_audio_processor() í˜¸ì¶œë¨")
    
    # ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
    existing_instance = InterviewAudioProcessor.get_instance()
    if existing_instance:
        print(f"â™»ï¸ [DEBUG] ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©: {id(existing_instance)}")
        return existing_instance
    
    # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    print(f"âœ¨ [DEBUG] ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
    return InterviewAudioProcessor()


def render_interview_live_page() -> None:
    """AI ë©´ì ‘ ì‹¤ì‹œê°„ í™”ë©´ ë Œë”ë§."""
    ctx = _init_context()
    started = st.session_state.get("interview_live_started", False)
    
    # íƒ€ì´ë¨¸ ìë™ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì£¼ê¸°ì  ìƒˆë¡œê³ ì¹¨ (ë©´ì ‘ ì§„í–‰ ì¤‘ì¼ ë•Œë§Œ)
    if started and ctx.get("interview_started"):
        # 5ì´ˆë§ˆë‹¤ íƒ€ì´ë¨¸ ì—…ë°ì´íŠ¸
        if "last_timer_update" not in st.session_state:
            st.session_state.last_timer_update = time.time()
        
        time_since_update = time.time() - st.session_state.last_timer_update
        if time_since_update >= 5.0:
            st.session_state.last_timer_update = time.time()
            time.sleep(0.1)
            st.rerun()

    st.markdown(
        """
        <style>
        .interview-frame {
            border:1px solid rgba(148,163,184,0.35);
            border-radius:16px;
            padding:16px;
            background:rgba(15,23,42,0.92);
            min-height:320px;
            display:flex;
            flex-direction:column;
            gap:12px;
            position:relative;
        }
        .interview-frame.ai::after {
            content:"AI ë©´ì ‘ê´€";
            position:absolute;
            top:12px;
            right:12px;
            font-size:0.8rem;
            color:#67e8f9;
            letter-spacing:0.05em;
        }
        .interview-frame.candidate::after {
            content:"ì§€ì›ì";
            position:absolute;
            top:12px;
            right:12px;
            font-size:0.8rem;
            color:#fbcfe8;
            letter-spacing:0.05em;
        }
        .webrtc-placeholder {
            flex:1;
            border-radius:12px;
            background:rgba(15,23,42,0.8);
            border:1px dashed rgba(148,163,184,0.5);
            display:flex;
            align-items:center;
            justify-content:center;
            color:#cbd5f5;
            font-size:0.95rem;
            text-align:center;
            padding:12px;
        }
        .ai-active {
            box-shadow:0 0 25px rgba(45,212,191,0.35);
            border-color:rgba(45,212,191,0.6) !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    if not started:
        _render_preflight_steps(ctx)
        return

    st.title("AI ëª¨ì˜ ë©´ì ‘ ëŒ€ì‹œë³´ë“œ")

    # ìƒë‹¨ ì œì–´ ë°”
    header_cols = st.columns([1.1, 2.0, 0.9])
    with header_cols[0]:
        origin = ctx.get("origin_nav", "status")
        if st.button("ğŸ”´ ë©´ì ‘ ì¢…ë£Œ", type="secondary", use_container_width=True):
            # í™•ì¸ ë‹¤ì´ì–¼ë¡œê·¸
            if not st.session_state.get("confirm_exit", False):
                st.session_state["confirm_exit"] = True
                st.warning("âš ï¸ ì •ë§ ë©´ì ‘ì„ ì¢…ë£Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? ì§„í–‰ ì¤‘ì¸ ë‹µë³€ì€ ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                confirm_cols = st.columns([1, 1])
                with confirm_cols[0]:
                    if st.button("âœ… ë„¤, ì¢…ë£Œí•©ë‹ˆë‹¤", type="primary", use_container_width=True):
                        # ë©´ì ‘ ì„¸ì…˜ì´ ìˆìœ¼ë©´ ì¤‘ê°„ ì €ì¥ ì‹œë„
                        if ctx.get("session_id") and ctx.get("interview_started"):
                            try:
                                result = _end_interview(ctx["session_id"])
                                if result and "interview_id" in result:
                                    st.success(f"ë¶€ë¶„ ë‹µë³€ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ID: {result['interview_id']})")
                            except:
                                pass  # ì‹¤íŒ¨í•´ë„ ì¢…ë£ŒëŠ” ì§„í–‰
                        
                        st.session_state["interview_live_started"] = False
                        st.session_state["confirm_exit"] = False
                        st.session_state["nav_selected_code"] = origin
                        st.rerun()
                with confirm_cols[1]:
                    if st.button("âŒ ì·¨ì†Œ", type="secondary", use_container_width=True):
                        st.session_state["confirm_exit"] = False
                        st.rerun()
                st.stop()
            else:
                st.session_state["confirm_exit"] = False

    with header_cols[1]:
        current = max(ctx["current_question"], 0)
        total = max(ctx["total_questions"], 1)
        progress_val = current / total if current > 0 else 0.0
        st.progress(progress_val, text=f"ì§„í–‰ë¥ : {current}/{total} ì§ˆë¬¸")
        
        if current > 0:
            question_preview = ctx.get("question_text", "")[:50] + "..." if len(ctx.get("question_text", "")) > 50 else ctx.get("question_text", "")
            st.metric("í˜„ì¬ ì§ˆë¬¸", f"Q{current:02d}", question_preview)
        else:
            st.info("ë©´ì ‘ì„ ì‹œì‘í•˜ë ¤ë©´ 'ë©´ì ‘ ì‹œì‘í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

    with header_cols[2]:
        # ë©´ì ‘ ì§„í–‰ ì¤‘ì¼ ë•Œë§Œ ì‹¤ì‹œê°„ íƒ€ì´ë¨¸ í‘œì‹œ
        if ctx.get("interview_started"):
            _render_countdown_timer(ctx.get("time_limit", 90), key=f"q{ctx['current_question']}")
        else:
            st.markdown(_render_timer_html(ctx.get("time_limit", 90)), unsafe_allow_html=True)

    st.markdown("---")

    # ë“€ì–¼ ë·°
    video_cols = st.columns([1.2, 0.05, 1.2])
    with video_cols[0]:
        st.subheader("ğŸ¤– AI ë©´ì ‘ê´€")
        st.markdown(
            """
            <div class="interview-frame ai ai-active">
                <div class="webrtc-placeholder">
                    WebRTC ì»´í¬ë„ŒíŠ¸ ìë¦¬<br/>
                    (AI ì•„ë°”íƒ€ ìŠ¤íŠ¸ë¦¼ / TTS ì¶œë ¥)
                </div>
                <div style="margin-top:8px;">
                    <p style="margin:0;font-weight:600;color:#67e8f9;">
                        í˜„ì¬ ì§ˆë¬¸ 
                        <span style="background:rgba(45,212,191,0.2);padding:2px 8px;border-radius:4px;font-size:0.8rem;margin-left:8px;">
                            {category}
                        </span>
                    </p>
                    <p style="margin:4px 0 0;color:#e0f2fe;font-size:1rem;">{question}</p>
                </div>
            </div>
            """.format(
                question=ctx.get("question_text", "ë©´ì ‘ì„ ì‹œì‘í•˜ë©´ ì§ˆë¬¸ì´ í‘œì‹œë©ë‹ˆë‹¤."),
                category=ctx.get("question_category", "ì¼ë°˜")
            ),
            unsafe_allow_html=True,
        )

    # WebRTC ë³€ìˆ˜ë¥¼ ë¸”ë¡ ë°”ê¹¥ì— ì„ ì–¸ (ìŠ¤ì½”í”„ ë¬¸ì œ í•´ê²°)
    webrtc_key = f"candidate-stream-{ctx.get('session_id', 'default')}"
    webrtc_ctx = None
    connection_ready = False
    
    with video_cols[2]:
        st.subheader("ğŸ‘¤ ì§€ì›ì")
        st.markdown(
            """
            <div class="interview-frame candidate">
                <div class="webrtc-placeholder">
                    ì§€ì›ì ì¹´ë©”ë¼/ë§ˆì´í¬ ì—°ê²° ì¤‘...
                </div>
                <p style="text-align:center;color:#cbd5f5;margin:8px 0 0;font-size:0.9rem;">
                    * ë³¸ì¸ì˜ ìì„¸, í‘œì •, ì‹œì„ ì„ í™•ì¸í•˜ë©° ë‹µë³€ì„ ì§„í–‰í•´ì£¼ì„¸ìš”.
                </p>
            </div>
            """,
            unsafe_allow_html=True,
        )
        
        webrtc_ctx = webrtc_streamer(
            key=webrtc_key,
            mode=WebRtcMode.SENDRECV,
            rtc_configuration=RTC_CONFIG,
            media_stream_constraints={
                "video": {"width": 640, "height": 480},
                "audio": {
                    "echoCancellation": True,
                    "noiseSuppression": True,
                    "autoGainControl": True,
                }
            },
            async_processing=True,
            audio_processor_factory=create_audio_processor,  # Factory í•¨ìˆ˜ ì‚¬ìš©
        )

        connection_ready = bool(webrtc_ctx and webrtc_ctx.state.playing)
        if connection_ready:
            st.success("âœ… ì¹´ë©”ë¼/ë§ˆì´í¬ ì—°ê²°ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            # í”„ë¡œì„¸ì„œ ìƒíƒœ í™•ì¸
            processor = InterviewAudioProcessor.get_instance()
            if processor:
                st.info(f"ğŸ¤ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ í™œì„±í™”ë¨ (í˜„ì¬ ë²„í¼: {len(processor._buffer)}ê°œ í”„ë ˆì„)")
            else:
                st.warning("âš ï¸ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª‡ ì´ˆ ê¸°ë‹¤ë¦° í›„ ë§ì”€í•´ì£¼ì„¸ìš”.")
        elif webrtc_ctx and hasattr(webrtc_ctx, 'state'):
            # WebRTC ì—°ê²° ìƒíƒœ ì„¸ë¶€ í™•ì¸
            state = webrtc_ctx.state
            if hasattr(state, 'signalling_state'):
                st.warning(f"âš ï¸ WebRTC ì—°ê²° ì¤‘... (ìƒíƒœ: {state.signalling_state})")
                st.caption("ğŸ’¡ ì—°ê²°ì´ ì˜¤ë˜ ê±¸ë¦¬ë©´ 'STOP' ë²„íŠ¼ì„ ëˆ„ë¥´ê³  'START'ë¥¼ ë‹¤ì‹œ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
            else:
                st.info("ì—°ê²° ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì ‘ê·¼ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.")
            st.caption("ğŸ’¡ íŒ: ë¸Œë¼ìš°ì € ì£¼ì†Œì°½ ì™¼ìª½ì˜ ì•„ì´ì½˜ì„ í´ë¦­í•´ ë§ˆì´í¬ ê¶Œí•œì„ 'í—ˆìš©'ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        else:
            st.warning("âš ï¸ WebRTC ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. 'START' ë²„íŠ¼ì„ ëˆŒëŸ¬ ì—°ê²°ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.")
            st.caption("ğŸ’¡ ì²« ì—°ê²° ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'STOP' í›„ 'START'ë¥¼ ë‹¤ì‹œ ëˆŒëŸ¬ë³´ì„¸ìš”.")

    st.markdown("---")

    # ì‹¤ì‹œê°„ STT
    st.subheader("ğŸ“ ì‹¤ì‹œê°„ ë‹µë³€ ê¸°ë¡ (STT)")
    st.caption("ğŸ‘‚ AIê°€ ì‚¬ìš©ìì˜ ë‹µë³€ì„ ì¸ì‹í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

    transcript_placeholder = st.empty()
    transcript_text = "\n".join(ctx.get("transcript") or ["(ì•„ì§ ìŒì„±ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.)"])
    transcript_placeholder.code(transcript_text, language="text")

    st.markdown(
        "<p style='color:#f87171;'>* ë‹µë³€ì´ ì™„ë£Œë˜ë©´ 'ë…¹ìŒ ì €ì¥ ë° STT' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”. (ë²„íŠ¼ì´ ë°˜ì‘í•˜ì§€ ì•Šìœ¼ë©´ í•œ ë²ˆ ë” í´ë¦­í•´ì£¼ì„¸ìš”)</p>",
        unsafe_allow_html=True,
    )

    st.markdown("#### ğŸ™ï¸ ë‹µë³€ ë…¹ìŒ / STT")
    col_rec, col_tts = st.columns([1, 1])
    with col_rec:
        record_disabled = not connection_ready
        
        # ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬
        if st.button("ğŸ’¾ ë…¹ìŒ ì €ì¥ ë° STT", use_container_width=True, disabled=record_disabled):
            # webrtc_ctxì—ì„œ audio_processor ê°€ì ¸ì˜¤ê¸°
            processor = None
            if hasattr(webrtc_ctx, "audio_processor"):
                processor = webrtc_ctx.audio_processor
                print(f"ğŸ” [DEBUG] webrtc_ctx.audio_processor: {processor}, type={type(processor)}")
            
            # webrtc_ctxì— ì—†ìœ¼ë©´ í´ë˜ìŠ¤ ë ˆë²¨ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
            if not processor:
                processor = InterviewAudioProcessor.get_instance()
                print(f"ğŸ” [DEBUG] InterviewAudioProcessor.get_instance(): {processor}, type={type(processor)}")
            
            if not isinstance(processor, InterviewAudioProcessor):
                st.error("âš ï¸ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.info(f"ë””ë²„ê·¸: webrtc_ctx íƒ€ì…={type(webrtc_ctx)}, processor={processor}")
                st.caption("í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ê³ , 'Start' ë²„íŠ¼ì„ ëˆ„ë¥¸ í›„ ëª‡ ì´ˆ ê¸°ë‹¤ë¦° í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            else:
                chunks, sample_rate, frame_count = processor.dump_audio()
                st.info(f"ğŸ” ë””ë²„ê·¸: ìˆ˜ì‹ ëœ í”„ë ˆì„ ìˆ˜={frame_count}, ë²„í¼ ì²­í¬={len(chunks)}, ìƒ˜í”Œë ˆì´íŠ¸={sample_rate}")
                
                if not chunks or not sample_rate:
                    st.warning(f"ìˆ˜ì‹ ëœ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ëª‡ ì´ˆê°„ ë§í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (í”„ë ˆì„ ì¹´ìš´íŠ¸: {frame_count})")
                    if frame_count == 0:
                        st.error("âš ï¸ ì˜¤ë””ì˜¤ í”„ë ˆì„ì´ ì „í˜€ ìˆ˜ì‹ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        st.caption("ğŸ”§ **ë””ë²„ê¹… íŒíŠ¸:**")
                        st.caption("1. í„°ë¯¸ë„ì— `ğŸ­ [DEBUG] create_audio_processor() í˜¸ì¶œë¨` ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸")
                        st.caption("2. í„°ë¯¸ë„ì— `ğŸ¤ [DEBUG] í”„ë ˆì„ ìˆ˜ì‹ ë¨` ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸")
                        st.caption("3. ë¸Œë¼ìš°ì € ì½˜ì†”(F12)ì—ì„œ WebRTC ê´€ë ¨ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸")
                else:
                    try:
                        file_path = _save_audio_chunks(chunks, sample_rate)
                    except ValueError as exc:
                        st.error(f"ë…¹ìŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {exc}")
                    else:
                        ctx["last_recording_path"] = str(file_path)
                        st.success(f"âœ… ë…¹ìŒ íŒŒì¼ ì €ì¥: {file_path.name} (í”„ë ˆì„ {frame_count}ê°œ, ì²­í¬ {len(chunks)}ê°œ)")
                        try:
                            text = transcribe_audio(file_path)
                            ctx["last_transcript"] = text
                            ctx.setdefault("transcript", []).append(text or "(ì¸ì‹ ì‹¤íŒ¨)")
                            transcript_placeholder.code("\n".join(ctx["transcript"]), language="text")
                            st.success(f"âœ… STT ì™„ë£Œ: {text[:100]}..." if len(text) > 100 else f"âœ… STT ì™„ë£Œ: {text}")
                            
                            # ë°±ì—”ë“œë¡œ ë‹µë³€ ì œì¶œ
                            if ctx.get("session_id") and ctx.get("interview_started"):
                                with st.spinner("ë‹µë³€ì„ ì œì¶œí•˜ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ì¤€ë¹„ ì¤‘..."):
                                    result = _submit_answer(ctx["session_id"], text)
                                
                                if result and "status" in result:
                                    if result["status"] == "continue":
                                        # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰
                                        ctx["question_text"] = result["next_question"]
                                        ctx["question_category"] = result.get("question_category", "ì¼ë°˜")
                                        ctx["current_question"] = result["current_question_num"]
                                        st.success(f"âœ… Q{ctx['current_question']}: {ctx['question_text'][:50]}...")
                                        st.rerun()
                                    elif result["status"] == "finished":
                                        # ë©´ì ‘ ì¢…ë£Œ
                                        st.success("ğŸ‰ ëª¨ë“  ì§ˆë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                                        ctx["interview_started"] = False
                                        
                                        # í‰ê°€ ê²°ê³¼ í‘œì‹œ
                                        if "evaluation" in result:
                                            st.json(result["evaluation"])
                                        
                                        # ë©´ì ‘ ì¢…ë£Œ ì²˜ë¦¬
                                        end_result = _end_interview(ctx["session_id"])
                                        if end_result and "interview_id" in end_result:
                                            ctx["interview_id"] = end_result["interview_id"]
                                            st.info(f"ë©´ì ‘ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ID: {end_result['interview_id']})")
                                else:
                                    st.warning("ë‹µë³€ ì œì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                        except Exception as exc:
                            st.error(f"STT ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {exc}")

    with col_tts:
        if st.button("ğŸ”Š AI ì§ˆë¬¸ ìŒì„± ì¬ìƒ", use_container_width=True):
            with st.spinner("ìŒì„±ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    question_text = ctx.get("question_text", "ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"ğŸ”Š [DEBUG] TTS ë²„íŠ¼ í´ë¦­: {question_text[:50]}...")
                    
                    # ë°±ì—”ë“œ API í˜¸ì¶œ
                    response = requests.post(
                        f"{BACKEND_URL}/api/v1/interview-live/tts",
                        json={"text": question_text},
                        timeout=30
                    )
                    response.raise_for_status()
                    
                    audio_bytes = response.content
                    print(f"âœ… [DEBUG] TTS ì‘ë‹µ ìˆ˜ì‹ : {len(audio_bytes)} bytes")
                    
                    # gTTSëŠ” MP3 í¬ë§·ìœ¼ë¡œ ì¶œë ¥
                    st.audio(audio_bytes, format="audio/mp3")
                    st.success("âœ… TTS ìŒì„± ì¬ìƒ ì™„ë£Œ")
                except requests.exceptions.Timeout:
                    st.error("TTS ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                except requests.exceptions.ConnectionError:
                    st.error("ë°±ì—”ë“œ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                except Exception as exc:
                    st.error(f"TTS ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {exc}")
                    print(f"âŒ [DEBUG] TTS ì˜¤ë¥˜: {exc}")

    action_cols = st.columns([1, 1])
    with action_cols[0]:
        st.button("â¸ï¸ ì¼ì‹œ ì •ì§€", use_container_width=True, disabled=True)
    with action_cols[1]:
        st.button("ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™ >>", type="primary", use_container_width=True, disabled=True)


def _save_audio_chunks(chunks: List[np.ndarray], sample_rate: int) -> Path:
    """AudioProcessorì—ì„œ ì¶”ì¶œí•œ numpy ë°°ì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥."""
    if not chunks or not sample_rate:
        raise ValueError("ì˜¤ë””ì˜¤ ë²„í¼ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ìƒ˜í”Œë ˆì´íŠ¸ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    channels = chunks[0].shape[0]
    total_samples = sum(chunk.shape[1] for chunk in chunks)
    if total_samples < sample_rate * 0.5:
        raise ValueError("ë…¹ìŒ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 0.5ì´ˆ ì´ìƒ ë§í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    audio_ndarray = np.concatenate(chunks, axis=1)
    audio_ndarray = audio_ndarray.transpose()
    audio_ndarray = np.clip(audio_ndarray, -32768, 32767).astype(np.int16)

    file_path = RECORDINGS_DIR / f"{uuid.uuid4().hex}.wav"
    with wave.open(str(file_path), "wb") as wf:
        wf.setnchannels(channels)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_ndarray.tobytes())

    return file_path

