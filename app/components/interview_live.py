from __future__ import annotations

import math
import uuid
import wave
from pathlib import Path
from collections import deque
from threading import Lock
from typing import Deque, Dict, Any, List, Optional, Tuple

import numpy as np
import streamlit as st
import sys
from streamlit_webrtc import (
    WebRtcMode,
    RTCConfiguration,
    webrtc_streamer,
    AudioProcessorBase,
)

PROJECT_ROOT = Path(__file__).resolve().parents[2]
SERVER_ROOT = PROJECT_ROOT / "server"
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
if str(SERVER_ROOT) not in sys.path:
    sys.path.insert(0, str(SERVER_ROOT))

from server.utils.openai_audio import synthesize_speech, transcribe_audio

RECORDINGS_DIR = SERVER_ROOT / "data" / "interview_recordings"
RECORDINGS_DIR.mkdir(parents=True, exist_ok=True)

RTC_CONFIG = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)


def _init_context() -> Dict[str, Any]:
    """Interview Live í™”ë©´ì—ì„œ ì‚¬ìš©í•  ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜."""
    default_context = {
        "interview_id": None,
        "application_id": None,
        "candidate_name": "ì§€ì›ì",
        "role": "candidate",
        "origin_nav": "status",
        "current_question": 1,
        "total_questions": 5,
        "question_text": "AI ë©´ì ‘ê´€ì´ ì²« ì§ˆë¬¸ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "time_limit": 90,
        "transcript": [],
        "last_recording_path": None,
        "last_transcript": "",
    }
    ctx = st.session_state.setdefault("interview_live_context", default_context)

    # default keys ë³´ì¥
    for key, value in default_context.items():
        ctx.setdefault(key, value)
    return ctx


def _render_preflight_steps(ctx: Dict[str, Any]) -> None:
    st.title("AI ë©´ì ‘ì„ ì‹œì‘í•˜ê¸° ì „ì—â€¦")
    st.caption("ì›í™œí•œ ë©´ì ‘ ì§„í–‰ì„ ìœ„í•´ ì•„ë˜ ë‹¨ê³„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    steps = [
        ("Step 1", "ì¹´ë©”ë¼ / ë§ˆì´í¬ ì—°ê²° í…ŒìŠ¤íŠ¸"),
        ("Step 2", "ë©´ì ‘ ê·œì¹™ í™•ì¸ (ì œí•œ ì‹œê°„, ì¬ì‹œë„ ë¶ˆê°€ ë“±)"),
        ("Step 3", "ì¡°ìš©í•œ í™˜ê²½ì—ì„œ ì§„í–‰ ì¤€ë¹„"),
    ]

    cols = st.columns(len(steps))
    for col, (title, desc) in zip(cols, steps):
        with col:
            st.markdown(
                f"""
                <div style="
                    border:1px solid rgba(148,163,184,0.4);
                    border-radius:12px;
                    padding:16px;
                    background:rgba(15,23,42,0.75);
                    min-height:150px;
                ">
                    <p style="font-weight:700;color:#f9fafb;font-size:1.05rem;margin-bottom:6px;">{title}</p>
                    <p style="color:#cbd5f5;font-size:0.9rem;line-height:1.4;">{desc}</p>
                </div>
                """,
                unsafe_allow_html=True,
            )

    st.markdown("---")
    button_cols = st.columns([1, 1])
    with button_cols[0]:
        if st.button("â†©ï¸ ì´ì „ í™”ë©´", type="secondary", use_container_width=True):
            origin = ctx.get("origin_nav", "status")
            st.session_state["interview_live_started"] = False
            st.session_state["nav_selected_code"] = origin
            st.rerun()
    with button_cols[1]:
        if st.button("âœ… ë©´ì ‘ ì‹œì‘í•˜ê¸°", type="primary", use_container_width=True):
            st.session_state["interview_live_started"] = True
            st.rerun()


def _render_timer_html(seconds: int) -> str:
    minutes = math.floor(seconds / 60)
    remain = seconds % 60
    return f"<h3 style='text-align: right; color: #38bdf8; margin:0;'>â± {minutes:02d}:{remain:02d}</h3>"


class InterviewAudioProcessor(AudioProcessorBase):
    """ì›¹ìº ì—ì„œ ìˆ˜ì‹ í•œ ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ ë²„í¼ì— ì ì¬í•˜ê³  í•„ìš” ì‹œ ì¶”ì¶œ."""

    # í´ë˜ìŠ¤ ë ˆë²¨ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
    _instance: Optional["InterviewAudioProcessor"] = None

    def __init__(self) -> None:
        super().__init__()  # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” ëª…ì‹œ
        self._buffer: Deque[np.ndarray] = deque(maxlen=1600)
        self._sample_rate: Optional[int] = None
        self._channels: Optional[int] = None
        self._lock = Lock()
        self._frame_count = 0  # ë””ë²„ê¹…ìš© ì¹´ìš´í„°
        InterviewAudioProcessor._instance = self
        print(f"ğŸ”§ [DEBUG] InterviewAudioProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨: {id(self)}")

    def recv(self, frame):
        """ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹  ë° ë²„í¼ ì €ì¥"""
        try:
            self._sample_rate = frame.sample_rate
            # PyAV AudioLayoutì—ì„œ ì±„ë„ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            if hasattr(frame.layout, 'channels'):
                self._channels = frame.layout.channels
            elif hasattr(frame.layout, 'nb_channels'):
                self._channels = frame.layout.nb_channels
            else:
                # ê¸°ë³¸ê°’: ìŠ¤í…Œë ˆì˜¤
                self._channels = 2
                
            arr = frame.to_ndarray()
            with self._lock:
                self._buffer.append(arr.copy())
                self._frame_count += 1
                # ì²˜ìŒ 10ê°œ í”„ë ˆì„ì€ ë¡œê·¸ ì¶œë ¥
                if self._frame_count <= 10:
                    print(f"ğŸ¤ [DEBUG] í”„ë ˆì„ ìˆ˜ì‹ ë¨ #{self._frame_count}: shape={arr.shape}, rate={self._sample_rate}, channels={self._channels}")
            return frame
        except Exception as e:
            print(f"âŒ [DEBUG] recv() ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            raise

    def dump_audio(self) -> Tuple[List[np.ndarray], int, int]:
        """ë²„í¼ì˜ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  í”„ë ˆì„ ì¹´ìš´íŠ¸ë„ ë°˜í™˜"""
        with self._lock:
            if not self._buffer:
                return [], 0, self._frame_count
            arrays = list(self._buffer)
            self._buffer.clear()
            frame_count = self._frame_count
            self._frame_count = 0  # ì¹´ìš´í„° ë¦¬ì…‹
        if not self._sample_rate:
            return [], 0, frame_count
        print(f"ğŸ“¤ [DEBUG] dump_audio() í˜¸ì¶œë¨: {len(arrays)}ê°œ ì²­í¬, {frame_count}ê°œ í”„ë ˆì„")
        return arrays, self._sample_rate, frame_count

    @classmethod
    def get_instance(cls) -> Optional["InterviewAudioProcessor"]:
        """í˜„ì¬ í™œì„±í™”ëœ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        return cls._instance


def create_audio_processor():
    """AudioProcessor Factory í•¨ìˆ˜"""
    print(f"ğŸ­ [DEBUG] create_audio_processor() í˜¸ì¶œë¨")
    return InterviewAudioProcessor()


def render_interview_live_page() -> None:
    """AI ë©´ì ‘ ì‹¤ì‹œê°„ í™”ë©´ ë Œë”ë§."""
    ctx = _init_context()
    started = st.session_state.get("interview_live_started", False)

    st.markdown(
        """
        <style>
        .interview-frame {
            border:1px solid rgba(148,163,184,0.35);
            border-radius:16px;
            padding:16px;
            background:rgba(15,23,42,0.92);
            min-height:320px;
            display:flex;
            flex-direction:column;
            gap:12px;
            position:relative;
        }
        .interview-frame.ai::after {
            content:"AI ë©´ì ‘ê´€";
            position:absolute;
            top:12px;
            right:12px;
            font-size:0.8rem;
            color:#67e8f9;
            letter-spacing:0.05em;
        }
        .interview-frame.candidate::after {
            content:"ì§€ì›ì";
            position:absolute;
            top:12px;
            right:12px;
            font-size:0.8rem;
            color:#fbcfe8;
            letter-spacing:0.05em;
        }
        .webrtc-placeholder {
            flex:1;
            border-radius:12px;
            background:rgba(15,23,42,0.8);
            border:1px dashed rgba(148,163,184,0.5);
            display:flex;
            align-items:center;
            justify-content:center;
            color:#cbd5f5;
            font-size:0.95rem;
            text-align:center;
            padding:12px;
        }
        .ai-active {
            box-shadow:0 0 25px rgba(45,212,191,0.35);
            border-color:rgba(45,212,191,0.6) !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    if not started:
        _render_preflight_steps(ctx)
        return

    st.title("AI ëª¨ì˜ ë©´ì ‘ ëŒ€ì‹œë³´ë“œ")

    # ìƒë‹¨ ì œì–´ ë°”
    header_cols = st.columns([1.1, 2.0, 0.9])
    with header_cols[0]:
        origin = ctx.get("origin_nav", "status")
        if st.button("ğŸ”´ ë©´ì ‘ ì¢…ë£Œ", type="secondary", use_container_width=True):
            st.session_state["interview_live_started"] = False
            st.session_state["nav_selected_code"] = origin
            st.rerun()

    with header_cols[1]:
        progress_val = ctx["current_question"] / max(ctx["total_questions"], 1)
        st.progress(progress_val, text=f"ì§„í–‰ë¥ : {ctx['current_question']}/{ctx['total_questions']} ì§ˆë¬¸")
        st.metric("í˜„ì¬ ì§ˆë¬¸", f"Q{ctx['current_question']:02d}", ctx.get("question_text", ""))

    with header_cols[2]:
        st.markdown(_render_timer_html(ctx.get("time_limit", 90)), unsafe_allow_html=True)

    st.markdown("---")

    # ë“€ì–¼ ë·°
    video_cols = st.columns([1.2, 0.05, 1.2])
    with video_cols[0]:
        st.subheader("ğŸ¤– AI ë©´ì ‘ê´€")
        st.markdown(
            """
            <div class="interview-frame ai ai-active">
                <div class="webrtc-placeholder">
                    WebRTC ì»´í¬ë„ŒíŠ¸ ìë¦¬<br/>
                    (AI ì•„ë°”íƒ€ ìŠ¤íŠ¸ë¦¼ / TTS ì¶œë ¥)
                </div>
                <div style="margin-top:8px;">
                    <p style="margin:0;font-weight:600;color:#67e8f9;">í˜„ì¬ ì§ˆë¬¸</p>
                    <p style="margin:4px 0 0;color:#e0f2fe;font-size:1rem;">{question}</p>
                </div>
            </div>
            """.format(question=ctx.get("question_text", "")),
            unsafe_allow_html=True,
        )

    with video_cols[2]:
        st.subheader("ğŸ‘¤ ì§€ì›ì")
        st.markdown(
            """
            <div class="interview-frame candidate">
                <div class="webrtc-placeholder">
                    ì§€ì›ì ì¹´ë©”ë¼/ë§ˆì´í¬ ì—°ê²° ì¤‘...
                </div>
                <p style="text-align:center;color:#cbd5f5;margin:8px 0 0;font-size:0.9rem;">
                    * ë³¸ì¸ì˜ ìì„¸, í‘œì •, ì‹œì„ ì„ í™•ì¸í•˜ë©° ë‹µë³€ì„ ì§„í–‰í•´ì£¼ì„¸ìš”.
                </p>
            </div>
            """,
            unsafe_allow_html=True,
        )
        webrtc_ctx = webrtc_streamer(
            key="candidate-stream",
            mode=WebRtcMode.SENDRECV,
            rtc_configuration=RTC_CONFIG,
            media_stream_constraints={
                "video": {"width": 640, "height": 480},
                "audio": {
                    "echoCancellation": True,
                    "noiseSuppression": True,
                    "autoGainControl": True,
                }
            },
            async_processing=True,
            audio_processor_factory=create_audio_processor,  # Factory í•¨ìˆ˜ ì‚¬ìš©
        )

        connection_ready = bool(webrtc_ctx and webrtc_ctx.state.playing)
        if connection_ready:
            st.success("âœ… ì¹´ë©”ë¼/ë§ˆì´í¬ ì—°ê²°ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            # í”„ë¡œì„¸ì„œ ìƒíƒœ í™•ì¸
            processor = InterviewAudioProcessor.get_instance()
            if processor:
                st.info(f"ğŸ¤ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ í™œì„±í™”ë¨ (í˜„ì¬ ë²„í¼: {len(processor._buffer)}ê°œ í”„ë ˆì„)")
            else:
                st.warning("âš ï¸ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª‡ ì´ˆ ê¸°ë‹¤ë¦° í›„ ë§ì”€í•´ì£¼ì„¸ìš”.")
        else:
            st.info("ì—°ê²° ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì ‘ê·¼ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.")
            st.caption("ğŸ’¡ íŒ: ë¸Œë¼ìš°ì € ì£¼ì†Œì°½ ì™¼ìª½ì˜ ì•„ì´ì½˜ì„ í´ë¦­í•´ ë§ˆì´í¬ ê¶Œí•œì„ 'í—ˆìš©'ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.")

    st.markdown("---")

    # ì‹¤ì‹œê°„ STT
    st.subheader("ğŸ“ ì‹¤ì‹œê°„ ë‹µë³€ ê¸°ë¡ (STT)")
    st.caption("ğŸ‘‚ AIê°€ ì‚¬ìš©ìì˜ ë‹µë³€ì„ ì¸ì‹í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

    transcript_placeholder = st.empty()
    transcript_text = "\n".join(ctx.get("transcript") or ["(ì•„ì§ ìŒì„±ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.)"])
    transcript_placeholder.code(transcript_text, language="text")

    st.markdown(
        "<p style='color:#f87171;'>* ë‹µë³€ì´ ì™„ë£Œë˜ë©´ 'ë…¹ìŒ ì €ì¥ ë° STT' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</p>",
        unsafe_allow_html=True,
    )

    st.markdown("#### ğŸ™ï¸ ë‹µë³€ ë…¹ìŒ / STT")
    col_rec, col_tts = st.columns([1, 1])
    with col_rec:
        record_disabled = not connection_ready
        if st.button("ğŸ’¾ ë…¹ìŒ ì €ì¥ ë° STT", use_container_width=True, disabled=record_disabled):
            # webrtc_ctxì—ì„œ audio_processor ê°€ì ¸ì˜¤ê¸°
            processor = None
            if hasattr(webrtc_ctx, "audio_processor"):
                processor = webrtc_ctx.audio_processor
                print(f"ğŸ” [DEBUG] webrtc_ctx.audio_processor: {processor}, type={type(processor)}")
            
            # webrtc_ctxì— ì—†ìœ¼ë©´ í´ë˜ìŠ¤ ë ˆë²¨ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
            if not processor:
                processor = InterviewAudioProcessor.get_instance()
                print(f"ğŸ” [DEBUG] InterviewAudioProcessor.get_instance(): {processor}, type={type(processor)}")
            
            if not isinstance(processor, InterviewAudioProcessor):
                st.error("âš ï¸ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.info(f"ë””ë²„ê·¸: webrtc_ctx íƒ€ì…={type(webrtc_ctx)}, processor={processor}")
                st.caption("í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ê³ , 'Start' ë²„íŠ¼ì„ ëˆ„ë¥¸ í›„ ëª‡ ì´ˆ ê¸°ë‹¤ë¦° í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            else:
                chunks, sample_rate, frame_count = processor.dump_audio()
                st.info(f"ğŸ” ë””ë²„ê·¸: ìˆ˜ì‹ ëœ í”„ë ˆì„ ìˆ˜={frame_count}, ë²„í¼ ì²­í¬={len(chunks)}, ìƒ˜í”Œë ˆì´íŠ¸={sample_rate}")
                
                if not chunks or not sample_rate:
                    st.warning(f"ìˆ˜ì‹ ëœ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ëª‡ ì´ˆê°„ ë§í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (í”„ë ˆì„ ì¹´ìš´íŠ¸: {frame_count})")
                    if frame_count == 0:
                        st.error("âš ï¸ ì˜¤ë””ì˜¤ í”„ë ˆì„ì´ ì „í˜€ ìˆ˜ì‹ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        st.caption("ğŸ”§ **ë””ë²„ê¹… íŒíŠ¸:**")
                        st.caption("1. í„°ë¯¸ë„ì— `ğŸ­ [DEBUG] create_audio_processor() í˜¸ì¶œë¨` ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸")
                        st.caption("2. í„°ë¯¸ë„ì— `ğŸ¤ [DEBUG] í”„ë ˆì„ ìˆ˜ì‹ ë¨` ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸")
                        st.caption("3. ë¸Œë¼ìš°ì € ì½˜ì†”(F12)ì—ì„œ WebRTC ê´€ë ¨ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸")
                else:
                    try:
                        file_path = _save_audio_chunks(chunks, sample_rate)
                    except ValueError as exc:
                        st.error(f"ë…¹ìŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {exc}")
                    else:
                        ctx["last_recording_path"] = str(file_path)
                        st.success(f"âœ… ë…¹ìŒ íŒŒì¼ ì €ì¥: {file_path.name} (í”„ë ˆì„ {frame_count}ê°œ, ì²­í¬ {len(chunks)}ê°œ)")
                        try:
                            text = transcribe_audio(file_path)
                            ctx["last_transcript"] = text
                            ctx.setdefault("transcript", []).append(text or "(ì¸ì‹ ì‹¤íŒ¨)")
                            transcript_placeholder.code("\n".join(ctx["transcript"]), language="text")
                            st.success("STT ê²°ê³¼ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        except Exception as exc:
                            st.error(f"STT ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {exc}")

    with col_tts:
        if st.button("ğŸ”Š AI ì§ˆë¬¸ ìŒì„± ì¬ìƒ", use_container_width=True):
            try:
                audio_bytes = synthesize_speech(ctx.get("question_text", "ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."))
                # gTTSëŠ” MP3 í¬ë§·ìœ¼ë¡œ ì¶œë ¥
                st.audio(audio_bytes, format="audio/mp3")
                st.success("âœ… TTS ìŒì„± ì¬ìƒ ì™„ë£Œ")
            except Exception as exc:
                st.error(f"TTS ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {exc}")

    action_cols = st.columns([1, 1])
    with action_cols[0]:
        st.button("â¸ï¸ ì¼ì‹œ ì •ì§€", use_container_width=True, disabled=True)
    with action_cols[1]:
        st.button("ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™ >>", type="primary", use_container_width=True, disabled=True)


def _save_audio_chunks(chunks: List[np.ndarray], sample_rate: int) -> Path:
    """AudioProcessorì—ì„œ ì¶”ì¶œí•œ numpy ë°°ì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥."""
    if not chunks or not sample_rate:
        raise ValueError("ì˜¤ë””ì˜¤ ë²„í¼ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ìƒ˜í”Œë ˆì´íŠ¸ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    channels = chunks[0].shape[0]
    total_samples = sum(chunk.shape[1] for chunk in chunks)
    if total_samples < sample_rate * 0.5:
        raise ValueError("ë…¹ìŒ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 0.5ì´ˆ ì´ìƒ ë§í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    audio_ndarray = np.concatenate(chunks, axis=1)
    audio_ndarray = audio_ndarray.transpose()
    audio_ndarray = np.clip(audio_ndarray, -32768, 32767).astype(np.int16)

    file_path = RECORDINGS_DIR / f"{uuid.uuid4().hex}.wav"
    with wave.open(str(file_path), "wb") as wf:
        wf.setnchannels(channels)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_ndarray.tobytes())

    return file_path

