from __future__ import annotations

import io
from pathlib import Path

from gtts import gTTS
from faster_whisper import WhisperModel

# Whisper ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ 1íšŒë§Œ)
_whisper_model = None


def _get_whisper_model():
    """Whisper ëª¨ë¸ì„ ë¡œë“œ (lazy loading)"""
    global _whisper_model
    if _whisper_model is None:
        print("ğŸ”„ [INFO] Faster-Whisper large-v3 ëª¨ë¸ ë¡œë”© ì¤‘...")
        # device: "cpu" ë˜ëŠ” "cuda", compute_type: "int8" (CPUìš©)
        _whisper_model = WhisperModel("large-v3", device="cpu", compute_type="int8")
        print("âœ… [INFO] Faster-Whisper large-v3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return _whisper_model


def synthesize_speech(text: str, *, lang: str = "ko") -> bytes:
    """Google TTS (gTTS) - ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš”"""
    print(f"ğŸ”Š [INFO] TTS ìƒì„± ì¤‘: {text[:50]}..." if len(text) > 50 else f"ğŸ”Š [INFO] TTS ìƒì„± ì¤‘: {text}")
    
    # gTTS ê°ì²´ ìƒì„±
    tts = gTTS(text=text, lang=lang, slow=False)
    
    # ë©”ëª¨ë¦¬ ë²„í¼ì— ì €ì¥
    audio_buffer = io.BytesIO()
    tts.write_to_fp(audio_buffer)
    audio_buffer.seek(0)
    
    audio_bytes = audio_buffer.read()
    print(f"âœ… [INFO] TTS ìƒì„± ì™„ë£Œ: {len(audio_bytes)} bytes")
    return audio_bytes


def transcribe_audio(file_path: str | Path) -> str:
    """ë¡œì»¬ Faster-Whisper large-v3 transcription (API í‚¤ ë¶ˆí•„ìš”)"""
    model = _get_whisper_model()
    segments, info = model.transcribe(str(file_path), language="ko", beam_size=5)
    
    # ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    text_parts = []
    for segment in segments:
        text_parts.append(segment.text)
    
    result = " ".join(text_parts).strip()
    print(f"ğŸ“ [INFO] STT ì™„ë£Œ: {result[:100]}..." if len(result) > 100 else f"ğŸ“ [INFO] STT ì™„ë£Œ: {result}")
    return result

